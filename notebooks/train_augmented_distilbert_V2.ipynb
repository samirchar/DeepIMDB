{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fce2ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4fce2ad",
    "outputId": "c70dac3a-0bfb-4323-c938-654962d596f4"
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers\n",
    "#!pip install -U datasets\n",
    "#!pip install optuna\n",
    "import os\n",
    "import sys\n",
    "HOME = os.path.abspath('..')\n",
    "sys.path.append(HOME)\n",
    "os.chdir(HOME)\n",
    "import pandas as pd\n",
    "#!pip install transformers\n",
    "from transformers import RobertaConfig, RobertaModel,RobertaForSequenceClassification, Trainer,AutoModelForSequenceClassification, EarlyStoppingCallback \n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.roberta import RobertaPreTrainedModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments\n",
    "import glob\n",
    "import optuna\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import csv\n",
    "from transformers import set_seed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image, ImageFilter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cpUQ_Pq4f6TK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpUQ_Pq4f6TK",
    "outputId": "e8c80ba2-e887-4880-d924-86aa08ddd84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model name:  distilbert-base-uncased-averageRating\n",
      "Saving at:  everything_as_text_and_images\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME =  \"distilbert-base-uncased\" #\"roberta-base\" \n",
    "TARGET_COL = 'averageRating'#'revenue_worldwide_BOM'\n",
    "MODEL_FOLDER = 'everything_as_text_and_images'#'everything_as_text'\n",
    "text_input_col = 'text_input'\n",
    "CATEGORIES_AS_TEXT = True\n",
    "NUMERIC_AS_TEXT = False\n",
    "DATE_AS_TEXT = False\n",
    "ADJUST_INFLATION = False\n",
    "USE_COLUMN_NAMES = False\n",
    "DEBUG = False\n",
    "IMG_SIZE = 224\n",
    "\n",
    "FINAL_MODEL_NAME = f\"{MODEL_NAME}-{TARGET_COL}\"\n",
    "\n",
    "if ADJUST_INFLATION:\n",
    "    FINAL_MODEL_NAME+='-inflation_adjusted'\n",
    "    \n",
    "if USE_COLUMN_NAMES:\n",
    "    FINAL_MODEL_NAME+='-with_column_names'\n",
    "    \n",
    "\n",
    "FINAL_MODEL_PATH = f'models/{MODEL_FOLDER}/{FINAL_MODEL_NAME}'\n",
    "TRIALS_DF_PATH = f'models/{MODEL_FOLDER}/{FINAL_MODEL_NAME}_hparams_trials.csv'\n",
    "TEST_PERFORMANCE_PATH = f'models/{MODEL_FOLDER}/{FINAL_MODEL_NAME}_test_stats_best_model.csv'\n",
    "    \n",
    "if USE_COLUMN_NAMES:\n",
    "    assert CATEGORIES_AS_TEXT|NUMERIC_AS_TEXT|DATE_AS_TEXT, \"can't use column names as text if there are no columns to treat as text!\"\n",
    "    \n",
    "print('Final model name: ',FINAL_MODEL_NAME)\n",
    "print('Saving at: ',MODEL_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1ebf2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "53c58fddfd8140a38c81c77f0726c864",
      "25b66346a56143cc921abe4556a9f6a3",
      "d3b13cad58694f2aa1400827e8a7f619",
      "c12944a1bd2541bbb210cb1c3585c855",
      "02e0d67806dc4a8e993cfd9f5a451bfa",
      "6e3b82dda5fa468d851c065a6cb538c0",
      "a43b2fb60dba45e4aa831ac03bb99323",
      "d009bc9fcbf54d089b779bd1adc4f49b",
      "06b2527e9e904b5199a36d2216033e25",
      "77b1c79a951d42d8a79ee3c472852192",
      "dacaefdf213d4b6ca041b7d491b94d42",
      "e127c609a823462aab9318fd04bda74b",
      "4b1d2bd0e14043f781d15615263b64ec",
      "518d709c96a14796b48c0ec8fdce9bbc",
      "feb6afab6c2247d48db7ba792d1daf85",
      "aa8df3ea2fb54cd2b9e2882e0649ee98",
      "29a5a5add01042c588faaf5751b5901e",
      "5c9ea04fc2524e6694b88fc6cda31ff8",
      "128d7ae29eb74e469d25af1941d13c7d",
      "f3023c4315d8401e8c8b886ae82ef3ca",
      "1d376fbfd41f42c9abf5206021234669",
      "d367c07d6c594fdf80e0bfc60a339504",
      "f4c6a7d98e284719999c9d4d2c5ff366",
      "7158cba3ef694c99a79339987df170c5",
      "b2cd9e361f404c15ab7b85e343f04176",
      "22fff9b519434cfe8644ee9478c23285",
      "7e42e1172adb461d871c7676afde511a",
      "6b5a84553cdf4c2abdc9f817f5c49c32",
      "7503acc7897e4264bcc1e50bd683da3a",
      "b834b852125147bdb5671574be4f1a0a",
      "2b15021c19ee4608941e9c340af1fc94",
      "a189fac46b0648d0924bbe6ac7b8036a",
      "15a7b28383b340bbb6201f026d642410",
      "0f0136838a2b44dfb71e8d7f98cc374a",
      "e32e63ab861449adb2f5d9a31d5785ff",
      "bd10fe03b50d4f2fb4911166e4219b18",
      "b32718625f18463ebb6df599fa5dc30c",
      "63ea088d311c4229b2f045cb2d1b5d15",
      "793ca2e7b3b24100ba6fa5551d44e03a",
      "b66319ac17e649aaa314f8e83cf7543c",
      "977047c3c9b0478fa2129dfb22504fb8",
      "9beb6465dfc3494aaafaec4d1c02fa12",
      "0ab61b2ec2054e61becc95bc2187b62d",
      "76eca6e8d5454b4a9693974954e60c9f"
     ]
    },
    "id": "1a1ebf2c",
    "outputId": "5a4b7506-ee0b-40da-f081-a56ce25839a8"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "\n",
    "def read_images_split(split,path = 'data/processed/posters/',id_col=\"imdb_id\"):\n",
    "  split_images = []\n",
    "  for row in split:\n",
    "\n",
    "      name = f'{int(row)}.jpg'\n",
    "\n",
    "      img_name = os.path.join(path,name)\n",
    "        \n",
    "      missing_image = plt.imread(os.path.join(path,'missing.jpg'))\n",
    "    \n",
    "      # Use you favourite library to load the image\n",
    "      try:\n",
    "        image = plt.imread(img_name)\n",
    "\n",
    "      except FileNotFoundError:\n",
    "        image = missing_image\n",
    "        \n",
    "      if len(image.shape)==2:\n",
    "        image = np.repeat(np.expand_dims(image,-1),3,-1)\n",
    "    \n",
    "      split_images.append(image)\n",
    "        \n",
    "  return split_images\n",
    "\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, transform):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = {}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        for key, val in self.encodings.items():\n",
    "            \n",
    "            if key == 'images':\n",
    "                item['images'] = Image.fromarray(val[idx].astype(np.uint8))\n",
    "                item['images'] = self.transform(item['images'])\n",
    "            else:\n",
    "                item[key] = torch.tensor(val[idx])\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def process_text_data(data_:pd.DataFrame,text_col,padding =\"max_length\", truncation = True, na_filler = \"\"):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    data = data_.copy()\n",
    "    data[text_col] = data[text_col].fillna(na_filler)\n",
    "    encodings = tokenizer(data[text_col].tolist(), padding=padding, truncation=truncation)\n",
    "    return encodings\n",
    "    \n",
    "\n",
    "def columns_to_single_text(df,\n",
    "                           cols_to_transform,\n",
    "                           new_col_name = 'text_input',\n",
    "                           sep = tokenizer.sep_token,\n",
    "                           nan_replacement = tokenizer.unk_token ):\n",
    "\n",
    "  '''\n",
    "  \n",
    "  Creates a new column called new_col_name with with all columns in cols_to_transform concatenated into a single text\n",
    "  '''\n",
    "  df[new_col_name] = df[cols_to_transform].astype(str).replace('nan',nan_replacement).agg(f' {sep} '.join, axis=1)\n",
    "\n",
    "\n",
    "class NAFiller:\n",
    "\n",
    "  def __init__(self,train):\n",
    "    self.train = train\n",
    "\n",
    "  def fit(self,column = 'Budget',groupby=['top_genre','top_country']):\n",
    "    self.mapping = self.train.groupby(groupby)[column].median().reset_index()\n",
    "    self.mapping = self.mapping.rename(columns={column:'na_filler'})\n",
    "    self.median = self.train[column].median()\n",
    "    self.column=column\n",
    "\n",
    "\n",
    "  def transform(self,test,round = False):\n",
    "    self.na_filler = test.merge(self.mapping,how='left')['na_filler']\n",
    "    self.na_filler = self.na_filler.fillna(self.median)\n",
    "\n",
    "    test[self.column] = test[self.column].reset_index(drop=True).fillna(self.na_filler).values\n",
    "\n",
    "    if round:\n",
    "      test[self.column] = test[self.column].round().astype(int)\n",
    "      \n",
    "\n",
    "\n",
    "  def fit_transform(self,test,column = 'Budget',groupby=['top_genre','top_country']):\n",
    "    self.fit(column,groupby)\n",
    "    self.transform()\n",
    "    self.column=column\n",
    "        \n",
    "\n",
    "def create_dataset_split(split,\n",
    "                         text_cols,\n",
    "                         text_input_col,\n",
    "                         TARGET_COL,\n",
    "                         transform,\n",
    "                         numeric_cols = [],\n",
    "                         images = None,\n",
    "                         new_col_name = 'text_input',\n",
    "                         sep = tokenizer.sep_token,\n",
    "                         nan_replacement = tokenizer.unk_token):\n",
    "    \n",
    "  if TARGET_COL == 'revenue_worldwide_BOM':\n",
    "    split[TARGET_COL] = np.log1p(split[TARGET_COL])\n",
    "    print('log transforming target')\n",
    "\n",
    "  #If all columns in text_cols are combined into a single text. A n\n",
    "  columns_to_single_text(split,text_cols)\n",
    "\n",
    "  #Get split encodings\n",
    "  split_encodings = process_text_data(split,text_input_col)\n",
    "    \n",
    "  if numeric_cols:\n",
    "    split_encodings['numeric_features'] = split[numeric_cols].values.tolist()\n",
    "    \n",
    "  if images:\n",
    "    split_encodings['images'] = images\n",
    "    \n",
    "    \n",
    "  #get labels\n",
    "  split_labels = split[TARGET_COL].tolist()\n",
    "\n",
    "  #Create dataset objects\n",
    "  split_dataset = IMDbDataset(split_encodings, split_labels,transform)\n",
    "\n",
    "  return split_dataset\n",
    "\n",
    "def date_to_season(doy):\n",
    "    doy = doy.dayofyear\n",
    "    # \"day of year\" ranges for the northern hemisphere\n",
    "    spring = range(80, 172)\n",
    "    summer = range(172, 264)\n",
    "    fall = range(264, 355)\n",
    "    # winter = everything else\n",
    "\n",
    "    if doy in spring:\n",
    "      season = 1 #'spring'\n",
    "    elif doy in summer:\n",
    "      season = 2 #'summer'\n",
    "    elif doy in fall:\n",
    "      season = 3 #'fall'\n",
    "    else:\n",
    "      season = 4 #'winter'\n",
    "    return season\n",
    "\n",
    "def cyclical_encoding(data, col, max_val, min_val = 1, drop = True):\n",
    "    \"\"\"Encoding of cyclical features using sine and cosine transformation.\n",
    "    Examples of cyclical features are: hour of day, month, day of week.\n",
    "\n",
    "    :param df: A dataframe containing the column we want to encode\n",
    "    :type df: :py:class:`pandas.DataFrame`\n",
    "    :param col: The name of the column we want to encode.\n",
    "    :type col: str\n",
    "    :param max_val: The maximum value the variable can have. e.g. in hour of day, max value = 23\n",
    "    :type max_val: int\n",
    "    :param min_val: The minimum value the variable can have. e.g. in hour of day, min value = 1, defaults to 1\n",
    "    :type min_val: int\n",
    "    :return: dataframe with three new variables: sine and cosine of the features + the multiplicationof these two columns\n",
    "    :rtype: :py:class:`pandas.DataFrame`\n",
    "    \"\"\"\n",
    "\n",
    "    data[col] = data[col] - min_val #ensure min value is 0\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n",
    "\n",
    "    if drop:\n",
    "        data.drop(col,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfa9287",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dfa9287",
    "outputId": "d8012d7f-61f3-4ce4-f2f1-04d708456137"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:91: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import RandAugment\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x\n",
    "    \n",
    "all_cols =  ['Budget',\n",
    "             'averageRating',\n",
    "             'cast',\n",
    "             'countries',\n",
    "             'director',\n",
    "             'genres',\n",
    "             'imdb_id',\n",
    "             'languages',\n",
    "             'overview',\n",
    "             'production companies',\n",
    "             'release_date',\n",
    "             'revenue_worldwide_BOM',\n",
    "             'runtimeMinutes',\n",
    "             'title']\n",
    "\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_train_augmented  =  transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomApply([\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "        ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "    #RandAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "    \n",
    "\n",
    "#Train/test transforms\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "    \n",
    "    \n",
    "    \n",
    "train_ids = pd.read_csv('data/processed/train.csv',usecols=['imdb_id'])['imdb_id'].tolist()\n",
    "val_ids = pd.read_csv('data/processed/val.csv',usecols=['imdb_id'])['imdb_id'].tolist()\n",
    "test_ids = pd.read_csv('data/processed/test.csv',usecols=['imdb_id'])['imdb_id'].tolist()\n",
    "df = pd.read_csv('data/processed/df.csv',usecols = all_cols,parse_dates=['release_date']).sample(frac=1,random_state=42) #shuffle\n",
    "\n",
    "\n",
    "\n",
    "#Additional auxilary columns\n",
    "df['top_genre'] = df['genres'].apply(lambda x: x.split(', ')[0])\n",
    "df['top_country'] = df['countries'].apply(lambda x: x.split(', ')[0] if isinstance(x,str) else x)\n",
    "\n",
    "\n",
    "categoric_cols = ['cast',\n",
    "                  'countries',\n",
    "                  'director',\n",
    "                  'genres',\n",
    "                  'languages',\n",
    "                  'production companies']\n",
    "\n",
    "text_cols = ['title','overview']                  \n",
    "date_cols = ['release_date']\n",
    "\n",
    "if (not DATE_AS_TEXT): #If date is not as text, include numeri date features\n",
    "    df['year'] = df['release_date'].dt.year\n",
    "    df['month'] = df['release_date'].dt.month\n",
    "    df['day'] = df['release_date'].dt.day\n",
    "\n",
    "\n",
    "    \n",
    "df[categoric_cols] = df[categoric_cols].apply(lambda x: x.str.replace('|',', '),axis=0) #Change pipe to comma, its more meaningful\n",
    "df['runtimeMinutes'] = pd.to_numeric(df['runtimeMinutes'],errors='coerce')\n",
    "\n",
    "\n",
    "numeric_cols = list(df.dtypes.index[(df.dtypes == int)|(df.dtypes == float)].drop(['imdb_id',\n",
    "                                                                              'averageRating',\n",
    "                                                                              'revenue_worldwide_BOM']))\n",
    "\n",
    "\n",
    "\n",
    "if CATEGORIES_AS_TEXT:\n",
    "  text_cols+=categoric_cols\n",
    "\n",
    "if NUMERIC_AS_TEXT:\n",
    "  text_cols+=numeric_cols\n",
    "\n",
    "if DATE_AS_TEXT:\n",
    "  text_cols+=date_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f979b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4f979b1",
    "outputId": "a6b275dd-9901-4562-8854-3c9f19652eeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = igetitem(value, i)\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = igetitem(value, i)\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    }
   ],
   "source": [
    "#Create splits\n",
    "if DEBUG:\n",
    "    train = df[df['imdb_id'].isin(train_ids)].sample(frac=0.2)\n",
    "    val = df[df['imdb_id'].isin(val_ids)].sample(frac=0.2)\n",
    "    test = df[df['imdb_id'].isin(test_ids)]\n",
    "else:\n",
    "    train = df[df['imdb_id'].isin(train_ids)]\n",
    "    val = df[df['imdb_id'].isin(val_ids)]\n",
    "    test = df[df['imdb_id'].isin(test_ids)]\n",
    "\n",
    "    \n",
    "#Get images per split\n",
    "train_images = read_images_split(train['imdb_id'].tolist())\n",
    "val_images = read_images_split(val['imdb_id'].tolist())\n",
    "test_images = read_images_split(test['imdb_id'].tolist())\n",
    "\n",
    "\n",
    "#Fill na in some columns with statistics\n",
    "naf = NAFiller(train)\n",
    "sc = StandardScaler()\n",
    "\n",
    "cols_to_impute = [i for i in numeric_cols if ('cos' not in i)&('sin' not in i)]\n",
    "    \n",
    "for col in cols_to_impute:\n",
    "    naf.fit(column = col,groupby=['top_genre','top_country'])\n",
    "    naf.transform(train,round=True)\n",
    "    naf.transform(val,round=True)\n",
    "    naf.transform(test,round=True)\n",
    "\n",
    "if not NUMERIC_AS_TEXT:\n",
    "    train[numeric_cols] = sc.fit_transform(train[numeric_cols])\n",
    "    val[numeric_cols] = sc.transform(val[numeric_cols])\n",
    "    test[numeric_cols] = sc.transform(test[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3cb2f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3cb2f40",
    "outputId": "df6bfe9e-0f23-441f-fc96-a81cd8831620"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_cols = numeric_cols if not NUMERIC_AS_TEXT else []\n",
    "train_dataset=create_dataset_split(train,text_cols,text_input_col,TARGET_COL,transform_train,numeric_cols,images = train_images)\n",
    "train_dataset_augmented=create_dataset_split(train,text_cols,text_input_col,TARGET_COL,transform_train_augmented,numeric_cols,images = train_images)\n",
    "\n",
    "val_dataset=create_dataset_split(val,text_cols,text_input_col,TARGET_COL,transform_test,numeric_cols,images = val_images)\n",
    "test_dataset=create_dataset_split(test,text_cols,text_input_col,TARGET_COL,transform_test,numeric_cols,images = test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9634cd-8dca-4dc4-996c-6f67bd7bf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a83335-24a5-410f-9c19-cc8a249c4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularConfig:\n",
    "    r\"\"\" Config used for tabular combiner\n",
    "    Args:\n",
    "        mlp_division (int): how much to decrease each MLP dim for each additional layer\n",
    "        combine_feat_method (str): The method to combine categorical and numerical features.\n",
    "            See :obj:`TabularFeatCombiner` for details on the supported methods.\n",
    "        mlp_dropout (float): dropout ratio used for MLP layers\n",
    "        numerical_bn (bool): whether to use batchnorm on numerical features\n",
    "        use_simple_classifier (bool): whether to use single layer or MLP as final classifier\n",
    "        mlp_act (str): the activation function to use for finetuning layers\n",
    "        gating_beta (float): the beta hyperparameters used for gating tabular data\n",
    "            see the paper `Integrating Multimodal Information in Large Pretrained Transformers <https://www.aclweb.org/anthology/2020.acl-main.214.pdf>`_ for details\n",
    "        numerical_feat_dim (int): the number of numerical features\n",
    "        cat_feat_dim (int): the number of categorical features\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_labels,\n",
    "                 mlp_division=4,\n",
    "                 combine_feat_method='text_only',\n",
    "                 mlp_dropout=0.1,\n",
    "                 numerical_bn=True,\n",
    "                 use_simple_classifier=True,\n",
    "                 mlp_act='relu',\n",
    "                 gating_beta=0.2,\n",
    "                 numerical_feat_dim=0,\n",
    "                 cat_feat_dim=0,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        self.mlp_division = mlp_division\n",
    "        self.combine_feat_method = combine_feat_method\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.numerical_bn = numerical_bn\n",
    "        self.use_simple_classifier = use_simple_classifier\n",
    "        self.mlp_act = mlp_act\n",
    "        self.gating_beta = gating_beta\n",
    "        self.numerical_feat_dim = numerical_feat_dim\n",
    "        self.cat_feat_dim = cat_feat_dim\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"mlp can specify number of hidden layers and hidden layer channels\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, act='relu', num_hidden_lyr=2,\n",
    "                 dropout_prob=0.5, return_layer_outs=False,\n",
    "                 hidden_channels=None, bn=False):\n",
    "        super().__init__()\n",
    "        self.out_dim = output_dim\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.return_layer_outs = return_layer_outs\n",
    "        if not hidden_channels:\n",
    "            hidden_channels = [input_dim for _ in range(num_hidden_lyr)]\n",
    "        elif len(hidden_channels) != num_hidden_lyr:\n",
    "            raise ValueError(\n",
    "                \"number of hidden layers should be the same as the lengh of hidden_channels\")\n",
    "        self.layer_channels = [input_dim] + hidden_channels + [output_dim]\n",
    "        self.act_name = act\n",
    "        self.activation = create_act(act)\n",
    "        self.layers = nn.ModuleList(list(\n",
    "            map(self.weight_init, [nn.Linear(self.layer_channels[i], self.layer_channels[i + 1])\n",
    "                                   for i in range(len(self.layer_channels) - 2)])))\n",
    "        final_layer = nn.Linear(self.layer_channels[-2], self.layer_channels[-1])\n",
    "        self.weight_init(final_layer,   activation='linear')\n",
    "        self.layers.append(final_layer)\n",
    "\n",
    "        self.bn = bn\n",
    "        if self.bn:\n",
    "            self.bn = nn.ModuleList([torch.nn.BatchNorm1d(dim) for dim in self.layer_channels[1:-1]])\n",
    "\n",
    "    def weight_init(self, m, activation=None):\n",
    "        if activation is None:\n",
    "            activation = self.act_name\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain(activation))\n",
    "        return m\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input features\n",
    "        :return: tuple containing output of MLP,\n",
    "                and list of inputs and outputs at every layer\n",
    "        \"\"\"\n",
    "        layer_inputs = [x]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            input = layer_inputs[-1]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer_inputs.append(layer(input))\n",
    "            else:\n",
    "                if self.bn:\n",
    "                    output = self.activation(self.bn[i](layer(input)))\n",
    "                else:\n",
    "                    output = self.activation(layer(input))\n",
    "                layer_inputs.append(self.dropout(output))\n",
    "\n",
    "        # model.store_layer_output(self, layer_inputs[-1])\n",
    "        if self.return_layer_outs:\n",
    "            return layer_inputs[-1], layer_inputs\n",
    "        else:\n",
    "            return layer_inputs[-1]\n",
    "\n",
    "\n",
    "def calc_mlp_dims(input_dim, division=2, output_dim=1):\n",
    "    dim = input_dim\n",
    "    dims = []\n",
    "    while dim > output_dim:\n",
    "        dim = dim // division\n",
    "        dims.append(int(dim))\n",
    "    dims = dims[:-1]\n",
    "    return dims\n",
    "\n",
    "\n",
    "def create_act(act, num_parameters=None):\n",
    "    if act == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act == 'prelu':\n",
    "        return nn.PReLU(num_parameters)\n",
    "    elif act == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif act == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act == 'linear':\n",
    "        class Identity(nn.Module):\n",
    "            def forward(self, x):\n",
    "                return x\n",
    "\n",
    "        return Identity()\n",
    "    else:\n",
    "        raise ValueError('Unknown activation function {}'.format(act))\n",
    "\n",
    "\n",
    "def glorot(tensor):\n",
    "    stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d8b815-d6a7-434f-be21-02c464b13d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from .layer_utils import calc_mlp_dims, create_act, glorot, zeros, MLP\n",
    "\n",
    "\n",
    "class TabularFeatCombiner(nn.Module):\n",
    "    r\"\"\"\n",
    "        Combiner module for combining text features with categorical and numerical features\n",
    "        The methods of combining, specified by :obj:`tabular_config.combine_feat_method` are shown below.\n",
    "        :math:`\\mathbf{m}` denotes the combined multimodal features,\n",
    "        :math:`\\mathbf{x}` denotes the output text features from the transformer,\n",
    "        :math:`\\mathbf{c}` denotes the categorical features, :math:`\\mathbf{t}` denotes the numerical features,\n",
    "        :math:`h_{\\mathbf{\\Theta}}` denotes a MLP parameterized by :math:`\\Theta`, :math:`W` denotes a weight matrix,\n",
    "        and :math:`b` denotes a scalar bias\n",
    "        - **text_only**\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\mathbf{x}\n",
    "        - **concat**\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\mathbf{x} \\, \\Vert \\, \\mathbf{c} \\, \\Vert \\, \\mathbf{n}\n",
    "        - **mlp_on_categorical_then_concat**\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\mathbf{x} \\, \\Vert \\, h_{\\mathbf{\\Theta}}( \\mathbf{c}) \\, \\Vert \\, \\mathbf{n}\n",
    "        - **individual_mlps_on_cat_and_numerical_feats_then_concat**\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\mathbf{x} \\, \\Vert \\, h_{\\mathbf{\\Theta_c}}( \\mathbf{c}) \\, \\Vert \\, h_{\\mathbf{\\Theta_n}}(\\mathbf{n})\n",
    "        - **mlp_on_concatenated_cat_and_numerical_feats_then_concat**\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\mathbf{x} \\, \\Vert \\, h_{\\mathbf{\\Theta}}( \\mathbf{c} \\, \\Vert \\, \\mathbf{n})\n",
    "        - **attention_on_cat_and_numerical_feats** self attention on the text features\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\alpha_{x,x}\\mathbf{W}_x\\mathbf{x} + \\alpha_{x,c}\\mathbf{W}_c\\mathbf{c} + \\alpha_{x,n}\\mathbf{W}_n\\mathbf{n}\n",
    "          where :math:`\\mathbf{W}_x` is of shape :obj:`(out_dim, text_feat_dim)`,\n",
    "          :math:`\\mathbf{W}_c` is of shape :obj:`(out_dim, cat_feat_dim)`,\n",
    "          :math:`\\mathbf{W}_n` is of shape :obj:`(out_dim, num_feat_dim)`, and the attention coefficients :math:`\\alpha_{i,j}` are computed as\n",
    "            .. math::\n",
    "                \\alpha_{i,j} =\n",
    "                \\frac{\n",
    "                \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "                [\\mathbf{W}_i\\mathbf{x}_i \\, \\Vert \\, \\mathbf{W}_j\\mathbf{x}_j]\n",
    "                \\right)\\right)}\n",
    "                {\\sum_{k \\in \\{ x, c, n \\}}\n",
    "                \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "                [\\mathbf{W}_i\\mathbf{x}_i \\, \\Vert \\, \\mathbf{W}_k\\mathbf{x}_k]\n",
    "                \\right)\\right)}.\n",
    "        - **gating_on_cat_and_num_feats_then_sum** sum of features gated by text features. Inspired by the gating mechanism introduced in `Integrating Multimodal Information in Large Pretrained Transformers <https://www.aclweb.org/anthology/2020.acl-main.214.pdf>`_\n",
    "            .. math::\n",
    "                \\mathbf{m}= \\mathbf{x} + \\alpha\\mathbf{h}\n",
    "            .. math::\n",
    "                \\mathbf{h} = \\mathbf{g_c} \\odot (\\mathbf{W}_c\\mathbf{c}) + \\mathbf{g_n} \\odot (\\mathbf{W}_n\\mathbf{n}) + b_h\n",
    "            .. math::\n",
    "                \\alpha = \\mathrm{min}( \\frac{\\| \\mathbf{x} \\|_2}{\\| \\mathbf{h} \\|_2}*\\beta, 1)\n",
    "          where :math:`\\beta` is a hyperparamter, :math:`\\mathbf{W}_c` is of shape :obj:`(out_dim, cat_feat_dim)`,\n",
    "          :math:`\\mathbf{W}_n` is of shape :obj:`(out_dim, num_feat_dim)`. and the gating vector :math:`\\mathbf{g}_i` with activation function :math:`R` is defined as\n",
    "            .. math::\n",
    "                \\mathbf{g}_i = R(\\mathbf{W}_{gi}[\\mathbf{i} \\, \\Vert \\, \\mathbf{x}]+ b_i)\n",
    "          where :math:`\\mathbf{W}_{gi}` is of shape :obj:`(out_dim, i_feat_dim + text_feat_dim)`\n",
    "        - **weighted_feature_sum_on_transformer_cat_and_numerical_feats**\n",
    "            .. math::\n",
    "                \\mathbf{m} = \\mathbf{x} + \\mathbf{W}_{c'} \\odot \\mathbf{W}_c \\mathbf{c} + \\mathbf{W}_{n'} \\odot \\mathbf{W}_n \\mathbf{t}\n",
    "       Parameters:\n",
    "           tabular_config (:class:`~multimodal_config.TabularConfig`):\n",
    "               Tabular model configuration class with all the parameters of the model.\n",
    "       \"\"\"\n",
    "    def __init__(self, tabular_config):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "\n",
    "        self.combine_feat_method = tabular_config.combine_feat_method\n",
    "        self.cat_feat_dim = tabular_config.cat_feat_dim\n",
    "        self.numerical_feat_dim = tabular_config.numerical_feat_dim\n",
    "        self.num_labels = tabular_config.num_labels\n",
    "        self.numerical_bn = tabular_config.numerical_bn\n",
    "        self.mlp_act = tabular_config.mlp_act\n",
    "        self.mlp_dropout = tabular_config.mlp_dropout\n",
    "        self.mlp_division = tabular_config.mlp_division\n",
    "        self.text_out_dim = tabular_config.text_feat_dim\n",
    "        self.tabular_config = tabular_config\n",
    "\n",
    "        if self.numerical_bn and self.numerical_feat_dim > 0:\n",
    "            self.num_bn = nn.BatchNorm1d(self.numerical_feat_dim)\n",
    "        else:\n",
    "            self.num_bn = None\n",
    "\n",
    "        if self.combine_feat_method == 'text_only':\n",
    "            self.final_out_dim = self.text_out_dim\n",
    "        elif self.combine_feat_method == 'concat':\n",
    "            self.final_out_dim = self.text_out_dim + self.cat_feat_dim \\\n",
    "                           + self.numerical_feat_dim\n",
    "        elif self.combine_feat_method == 'mlp_on_categorical_then_concat':\n",
    "            assert self.cat_feat_dim != 0, 'dimension of cat feats should not be 0'\n",
    "            # reduce dim of categorical features to same of num dim or text dim if necessary\n",
    "            output_dim = min(self.text_out_dim,\n",
    "                             max(self.numerical_feat_dim, self.cat_feat_dim // (self.mlp_division // 2)))\n",
    "            dims = calc_mlp_dims(\n",
    "                self.cat_feat_dim,\n",
    "                self.mlp_division,\n",
    "                output_dim\n",
    "            )\n",
    "            self.cat_mlp = MLP(\n",
    "                self.cat_feat_dim,\n",
    "                output_dim,\n",
    "                act=self.mlp_act,\n",
    "                num_hidden_lyr=len(dims),\n",
    "                dropout_prob=self.mlp_dropout,\n",
    "                hidden_channels=dims,\n",
    "                return_layer_outs=False,\n",
    "                bn=True\n",
    "            )\n",
    "            self.final_out_dim = self.text_out_dim + output_dim + self.numerical_feat_dim\n",
    "        elif self.combine_feat_method == 'mlp_on_concatenated_cat_and_numerical_feats_then_concat':\n",
    "            assert self.cat_feat_dim != 0, 'dimension of cat feats should not be 0'\n",
    "            assert self.numerical_feat_dim != 0, 'dimension of numerical feats should not be 0'\n",
    "            output_dim = min(self.numerical_feat_dim, self.cat_feat_dim, self.text_out_dim)\n",
    "            in_dim = self.cat_feat_dim + self.numerical_feat_dim\n",
    "            dims = calc_mlp_dims(\n",
    "                in_dim,\n",
    "                self.mlp_division,\n",
    "                output_dim\n",
    "            )\n",
    "            self.cat_and_numerical_mlp = MLP(\n",
    "                in_dim,\n",
    "                output_dim,\n",
    "                act=self.mlp_act,\n",
    "                num_hidden_lyr=len(dims),\n",
    "                dropout_prob=self.mlp_dropout,\n",
    "                hidden_channels=dims,\n",
    "                return_layer_outs=False,\n",
    "                bn=True\n",
    "            )\n",
    "            self.final_out_dim = self.text_out_dim + output_dim\n",
    "        elif self.combine_feat_method == 'individual_mlps_on_cat_and_numerical_feats_then_concat':\n",
    "            output_dim_cat = 0\n",
    "            if self.cat_feat_dim > 0:\n",
    "                output_dim_cat = max(self.cat_feat_dim // (self.mlp_division // 2),\n",
    "                                     self.numerical_feat_dim)\n",
    "                dims = calc_mlp_dims(\n",
    "                    self.cat_feat_dim,\n",
    "                    self.mlp_division,\n",
    "                    output_dim_cat)\n",
    "                self.cat_mlp = MLP(\n",
    "                    self.cat_feat_dim,\n",
    "                    output_dim_cat,\n",
    "                    act=self.mlp_act,\n",
    "                    num_hidden_lyr=len(dims),\n",
    "                    dropout_prob=self.mlp_dropout,\n",
    "                    hidden_channels=dims,\n",
    "                    return_layer_outs=False,\n",
    "                    bn=True)\n",
    "\n",
    "            output_dim_num = 0\n",
    "            if self.numerical_feat_dim > 0:\n",
    "                output_dim_num = self.numerical_feat_dim // (self.mlp_division // 2)\n",
    "                self.num_mlp = MLP(\n",
    "                    self.numerical_feat_dim,\n",
    "                    output_dim_num,\n",
    "                    act=self.mlp_act,\n",
    "                    dropout_prob=self.mlp_dropout,\n",
    "                    num_hidden_lyr=1,\n",
    "                    return_layer_outs=False,\n",
    "                    bn=True)\n",
    "            self.final_out_dim = self.text_out_dim + output_dim_num + output_dim_cat\n",
    "        elif self.combine_feat_method == 'weighted_feature_sum_on_transformer_cat_and_numerical_feats':\n",
    "            assert self.cat_feat_dim + self.numerical_feat_dim != 0, 'should have some non text features'\n",
    "            if self.cat_feat_dim > 0:\n",
    "                output_dim_cat = self.text_out_dim\n",
    "                if self.cat_feat_dim > self.text_out_dim:\n",
    "                    dims = calc_mlp_dims(\n",
    "                        self.cat_feat_dim,\n",
    "                        division=self.mlp_division,\n",
    "                        output_dim=output_dim_cat)\n",
    "                    self.cat_layer = MLP(\n",
    "                        self.cat_feat_dim,\n",
    "                        output_dim_cat,\n",
    "                        act=self.mlp_act,\n",
    "                        num_hidden_lyr=len(dims),\n",
    "                        dropout_prob=self.mlp_dropout,\n",
    "                        hidden_channels=dims,\n",
    "                        return_layer_outs=False,\n",
    "                        bn=True)\n",
    "                else:\n",
    "                    self.cat_layer = nn.Linear(self.cat_feat_dim, output_dim_cat)\n",
    "                self.dropout_cat = nn.Dropout(self.mlp_dropout)\n",
    "                self.weight_cat = nn.Parameter(torch.rand(output_dim_cat))\n",
    "            if self.numerical_feat_dim > 0:\n",
    "                output_dim_num = self.text_out_dim\n",
    "                if self.numerical_feat_dim > self.text_out_dim:\n",
    "                    dims = calc_mlp_dims(\n",
    "                        self.numerical_feat_dim,\n",
    "                        division=self.mlp_division,\n",
    "                        output_dim=output_dim_num)\n",
    "                    self.num_layer = MLP(\n",
    "                        self.numerical_feat_dim,\n",
    "                        output_dim_num,\n",
    "                        act=self.mlp_act,\n",
    "                        num_hidden_lyr=len(dims),\n",
    "                        dropout_prob=self.mlp_dropout,\n",
    "                        hidden_channels=dims,\n",
    "                        return_layer_outs=False,\n",
    "                        bn=True)\n",
    "                else:\n",
    "                    self.num_layer = nn.Linear(self.numerical_feat_dim,\n",
    "                                               output_dim_num)\n",
    "                self.dropout_num = nn.Dropout(self.mlp_dropout)\n",
    "                self.weight_num = nn.Parameter(torch.rand(output_dim_num))\n",
    "\n",
    "            self.act_func = create_act(self.mlp_act)\n",
    "            self.layer_norm = nn.LayerNorm(self.text_out_dim)\n",
    "            self.final_dropout = nn.Dropout(tabular_config.hidden_dropout_prob)\n",
    "            self.final_out_dim = self.text_out_dim\n",
    "\n",
    "        elif self.combine_feat_method == 'attention_on_cat_and_numerical_feats':\n",
    "            assert self.cat_feat_dim + self.numerical_feat_dim != 0, \\\n",
    "                'should have some non-text features for this method'\n",
    "\n",
    "            output_dim = self.text_out_dim\n",
    "            if self.cat_feat_dim > 0:\n",
    "                if self.cat_feat_dim > self.text_out_dim:\n",
    "                    output_dim_cat = self.text_out_dim\n",
    "                    dims = calc_mlp_dims(\n",
    "                        self.cat_feat_dim,\n",
    "                        division=self.mlp_division,\n",
    "                        output_dim=output_dim_cat)\n",
    "                    self.cat_mlp = MLP(\n",
    "                        self.cat_feat_dim,\n",
    "                        output_dim_cat,\n",
    "                        num_hidden_lyr=len(dims),\n",
    "                        dropout_prob=self.mlp_dropout,\n",
    "                        return_layer_outs=False,\n",
    "                        hidden_channels=dims,\n",
    "                        bn=True)\n",
    "                else:\n",
    "                    output_dim_cat = self.cat_feat_dim\n",
    "                self.weight_cat = nn.Parameter(torch.rand((output_dim_cat,\n",
    "                                                           output_dim)))\n",
    "                self.bias_cat = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "            if self.numerical_feat_dim > 0:\n",
    "                if self.numerical_feat_dim > self.text_out_dim:\n",
    "                    output_dim_num = self.text_out_dim\n",
    "                    dims = calc_mlp_dims(\n",
    "                        self.numerical_feat_dim,\n",
    "                        division=self.mlp_division,\n",
    "                        output_dim=output_dim_num)\n",
    "                    self.cat_mlp = MLP(\n",
    "                        self.numerical_feat_dim,\n",
    "                        output_dim_num,\n",
    "                        num_hidden_lyr=len(dims),\n",
    "                        dropout_prob=self.mlp_dropout,\n",
    "                        return_layer_outs=False,\n",
    "                        hidden_channels=dims,\n",
    "                        bn=True)\n",
    "                else:\n",
    "                    output_dim_num = self.numerical_feat_dim\n",
    "                self.weight_num = nn.Parameter(torch.rand((output_dim_num,\n",
    "                                                            output_dim)))\n",
    "                self.bias_num = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "            self.weight_transformer = nn.Parameter(torch.rand(self.text_out_dim,\n",
    "                                                       output_dim))\n",
    "            self.weight_a = nn.Parameter(torch.rand((1, output_dim + output_dim)))\n",
    "            self.bias_transformer = nn.Parameter(torch.rand(output_dim))\n",
    "            self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "            self.negative_slope = 0.2\n",
    "            self.final_out_dim = output_dim\n",
    "            self.__reset_parameters()\n",
    "        elif self.combine_feat_method == 'gating_on_cat_and_num_feats_then_sum':\n",
    "            self.act_func = create_act(self.mlp_act)\n",
    "            if self.cat_feat_dim > 0:\n",
    "                if self.cat_feat_dim > self.text_out_dim:\n",
    "                    dims = calc_mlp_dims(\n",
    "                        self.numerical_feat_dim,\n",
    "                        division=self.mlp_division,\n",
    "                        output_dim=self.text_out_dim)\n",
    "                    self.cat_layer = MLP(\n",
    "                        self.cat_feat_dim,\n",
    "                        self.text_out_dim,\n",
    "                        act=self.mlp_act,\n",
    "                        num_hidden_lyr=len(dims),\n",
    "                        dropout_prob=self.mlp_dropout,\n",
    "                        hidden_channels=dims,\n",
    "                        return_layer_outs=False,\n",
    "                        bn=True)\n",
    "                self.g_cat_layer = nn.Linear(self.text_out_dim + min(self.text_out_dim, self.cat_feat_dim),\n",
    "                                             self.text_out_dim)\n",
    "                self.dropout_cat = nn.Dropout(self.mlp_dropout)\n",
    "                self.h_cat_layer = nn.Linear(min(self.text_out_dim, self.cat_feat_dim), self.text_out_dim, bias=False)\n",
    "            if self.numerical_feat_dim > 0:\n",
    "                if self.numerical_feat_dim > self.text_out_dim:\n",
    "                    dims = calc_mlp_dims(\n",
    "                        self.numerical_feat_dim,\n",
    "                        division=self.mlp_division,\n",
    "                        output_dim=self.text_out_dim)\n",
    "                    self.num_layer = MLP(\n",
    "                        self.numerical_feat_dim,\n",
    "                        self.text_out_dim,\n",
    "                        act=self.mlp_act,\n",
    "                        num_hidden_lyr=len(dims),\n",
    "                        dropout_prob=self.mlp_dropout,\n",
    "                        hidden_channels=dims,\n",
    "                        return_layer_outs=False,\n",
    "                        bn=True)\n",
    "                self.g_num_layer = nn.Linear(min(self.numerical_feat_dim, self.text_out_dim) + self.text_out_dim,\n",
    "                                             self.text_out_dim)\n",
    "                self.dropout_num = nn.Dropout(self.mlp_dropout)\n",
    "                self.h_num_layer = nn.Linear(min(self.text_out_dim, self.numerical_feat_dim),\n",
    "                                             self.text_out_dim, bias=False)\n",
    "            self.h_bias = nn.Parameter(torch.zeros(self.text_out_dim))\n",
    "            self.layer_norm = nn.LayerNorm(self.text_out_dim)\n",
    "            self.final_out_dim = self.text_out_dim\n",
    "        else:\n",
    "            raise ValueError(f'combine_feat_method {self.combine_feat_method} '\n",
    "                             f'not implemented')\n",
    "\n",
    "    def forward(self, text_feats, cat_feats=None, numerical_feats=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_feats (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, text_out_dim)`):\n",
    "                The tensor of text features. This is assumed to be the output from a HuggingFace transformer model\n",
    "            cat_feats (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, cat_feat_dim)`, `optional`, defaults to :obj:`None`)):\n",
    "                The tensor of categorical features\n",
    "            numerical_feats (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, numerical_feat_dim)`, `optional`, defaults to :obj:`None`):\n",
    "                The tensor of numerical features\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, final_out_dim)`:\n",
    "                A tensor representing the combined features\n",
    "        \"\"\"\n",
    "        if cat_feats is None:\n",
    "            cat_feats = torch.zeros((text_feats.shape[0], 0)).to(text_feats.device)\n",
    "        if numerical_feats is None:\n",
    "            numerical_feats = torch.zeros((text_feats.shape[0], 0)).to(text_feats.device)\n",
    "\n",
    "        if self.numerical_bn and self.numerical_feat_dim != 0:\n",
    "            numerical_feats = self.num_bn(numerical_feats)\n",
    "\n",
    "        if self.combine_feat_method == 'text_only':\n",
    "            combined_feats = text_feats\n",
    "        if self.combine_feat_method == 'concat':\n",
    "            combined_feats = torch.cat((text_feats, cat_feats, numerical_feats),\n",
    "                                       dim=1)\n",
    "        elif self.combine_feat_method == 'mlp_on_categorical_then_concat':\n",
    "            cat_feats = self.cat_mlp(cat_feats)\n",
    "            combined_feats = torch.cat((text_feats, cat_feats, numerical_feats), dim=1)\n",
    "        elif self.combine_feat_method == 'mlp_on_concatenated_cat_and_numerical_feats_then_concat':\n",
    "            tabular_feats = torch.cat((cat_feats, numerical_feats), dim=1)\n",
    "            tabular_feats = self.cat_and_numerical_mlp(tabular_feats)\n",
    "            combined_feats = torch.cat((text_feats, tabular_feats), dim=1)\n",
    "        elif self.combine_feat_method == 'individual_mlps_on_cat_and_numerical_feats_then_concat':\n",
    "            if cat_feats.shape[1] != 0:\n",
    "                cat_feats = self.cat_mlp(cat_feats)\n",
    "            if numerical_feats.shape[1] != 0:\n",
    "                numerical_feats = self.num_mlp(numerical_feats)\n",
    "            combined_feats = torch.cat((text_feats, cat_feats, numerical_feats), dim=1)\n",
    "        elif self.combine_feat_method == 'weighted_feature_sum_on_transformer_cat_and_numerical_feats':\n",
    "            if cat_feats.shape[1] != 0:\n",
    "                cat_feats = self.dropout_cat(self.cat_layer(cat_feats))\n",
    "                cat_feats = self.weight_cat.expand_as(cat_feats) * cat_feats\n",
    "            else:\n",
    "                cat_feats = 0\n",
    "\n",
    "            if numerical_feats.shape[1] != 0:\n",
    "                numerical_feats = self.dropout_num(self.num_layer(numerical_feats))\n",
    "                numerical_feats = self.weight_num.expand_as(numerical_feats) * numerical_feats\n",
    "            else:\n",
    "                numerical_feats = 0\n",
    "            combined_feats = text_feats + cat_feats + numerical_feats\n",
    "        elif self.combine_feat_method == 'attention_on_cat_and_numerical_feats':\n",
    "            # attention keyed by transformer text features\n",
    "            w_text = torch.mm(text_feats, self.weight_transformer)\n",
    "            g_text = (torch.cat([w_text, w_text], dim=-1) * self.weight_a).sum(dim=1).unsqueeze(0).T\n",
    "\n",
    "            if cat_feats.shape[1] != 0:\n",
    "                if self.cat_feat_dim > self.text_out_dim:\n",
    "                    cat_feats = self.cat_mlp(cat_feats)\n",
    "                w_cat = torch.mm(cat_feats, self.weight_cat)\n",
    "                g_cat = (torch.cat([w_text, w_cat], dim=-1) * self.weight_a).sum(dim=1).unsqueeze(0).T\n",
    "            else:\n",
    "                w_cat = None\n",
    "                g_cat = torch.zeros(0, device=g_text.device)\n",
    "\n",
    "            if numerical_feats.shape[1] != 0:\n",
    "                if self.numerical_feat_dim > self.text_out_dim:\n",
    "                    numerical_feats = self.num_mlp(numerical_feats)\n",
    "                w_num = torch.mm(numerical_feats, self.weight_num)\n",
    "                g_num = (torch.cat([w_text, w_cat], dim=-1) * self.weight_a).sum(dim=1).unsqueeze(0).T\n",
    "            else:\n",
    "                w_num = None\n",
    "                g_num = torch.zeros(0, device=g_text.device)\n",
    "\n",
    "            alpha = torch.cat([g_text, g_cat, g_num], dim=1)  # N by 3\n",
    "            alpha = F.leaky_relu(alpha, 0.02)\n",
    "            alpha = F.softmax(alpha, -1)\n",
    "            stack_tensors = [tensor for tensor in [w_text, w_cat, w_num]\n",
    "                             if tensor is not None]\n",
    "            combined = torch.stack(stack_tensors, dim=1)  # N by 3 by final_out_dim\n",
    "            outputs_w_attention = alpha[:, :, None] * combined\n",
    "            combined_feats = outputs_w_attention.sum(dim=1)  # N by final_out_dim\n",
    "        elif self.combine_feat_method == 'gating_on_cat_and_num_feats_then_sum':\n",
    "            # assumes shifting of features relative to text features and that text features are the most important\n",
    "            if cat_feats.shape[1] != 0:\n",
    "                if self.cat_feat_dim > self.text_out_dim:\n",
    "                    cat_feats = self.cat_layer(cat_feats)\n",
    "                g_cat = self.dropout_cat(self.act_func(self.g_cat_layer(torch.cat([text_feats, cat_feats], dim=1))))\n",
    "                g_mult_cat = g_cat * self.h_cat_layer(cat_feats)\n",
    "            else:\n",
    "                g_mult_cat = 0\n",
    "\n",
    "            if numerical_feats.shape[1] != 0:\n",
    "                if self.numerical_feat_dim > self.text_out_dim:\n",
    "                    numerical_feats = self.num_layer(numerical_feats)\n",
    "                g_num = self.dropout_num(self.act_func(self.g_num_layer(torch.cat([text_feats, numerical_feats], dim=1))))\n",
    "                g_mult_num = g_num * self.h_num_layer(numerical_feats)\n",
    "            else:\n",
    "                g_mult_num = 0\n",
    "\n",
    "            H = g_mult_cat + g_mult_num + self.h_bias\n",
    "            norm = torch.norm(text_feats, dim=1) / torch.norm(H, dim=1)\n",
    "            alpha = torch.clamp(norm * self.tabular_config.gating_beta, min=0, max=1)\n",
    "            combined_feats = text_feats + alpha[:, None] * H\n",
    "\n",
    "        return combined_feats\n",
    "\n",
    "    def __reset_parameters(self):\n",
    "        glorot(self.weight_a)\n",
    "        if hasattr(self, 'weight_cat'):\n",
    "            glorot(self.weight_cat)\n",
    "            zeros(self.bias_cat)\n",
    "        if hasattr(self, 'weight_num'):\n",
    "            glorot(self.weight_num)\n",
    "            zeros(self.bias_num)\n",
    "        glorot(self.weight_transformer)\n",
    "        zeros(self.bias_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca4ba22-0707-4da7-980c-0d0a4f74a516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.modeling_distilbert import *\n",
    "\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "            \n",
    "class AugmentedDistilBertForSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        tabular_config = config.tabular_config\n",
    "        \n",
    "\n",
    "        \n",
    "        if type(tabular_config) is dict:  # when loading from saved model\n",
    "            tabular_config = TabularConfig(**tabular_config)\n",
    "        else:\n",
    "            self.config.tabular_config = tabular_config.__dict__\n",
    "            \n",
    "        tabular_config.text_feat_dim = config.hidden_size\n",
    "        tabular_config.hidden_dropout_prob = config.seq_classif_dropout\n",
    "        \n",
    "        self.tabular_combiner = TabularFeatCombiner(tabular_config)\n",
    "        \n",
    "        self.total_num_features = config.dim + config.num_extra_features\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "    \n",
    "        self.distilbert = DistilBertModel(self.config)\n",
    "        self.ln = nn.LayerNorm(self.total_num_features,eps=1e-12,elementwise_affine=True)\n",
    "        \n",
    "        \n",
    "        output_mlp_hidden_dim = self.total_num_features if self.config.concat_mode=='cls' else config.dim\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(output_mlp_hidden_dim, output_mlp_hidden_dim)\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Linear(self.total_num_features, self.config.num_labels)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.seq_classif_dropout)\n",
    "        \n",
    "        \n",
    "        self.image_model = models.resnet18(pretrained=True)\n",
    "        #set_parameter_requires_grad(self.model, feature_extract)\n",
    "        num_ftrs = self.image_model.fc.in_features\n",
    "        \n",
    "        \n",
    "        \n",
    "        if num_ftrs == self.config.num_image_features: #Then it doesn't make sense to add additional layer to reduce dim\n",
    "            self.image_model.fc = nn.Sequential(nn.Dropout(self.config.resnet_dropout),\n",
    "                                               )\n",
    "        else:\n",
    "            self.image_model.fc = nn.Sequential(nn.Dropout(self.config.resnet_dropout),\n",
    "                                                nn.Linear(num_ftrs, self.config.num_image_features),\n",
    "                                               )\n",
    "        \n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        numeric_features: Optional[torch.Tensor] = None,\n",
    "        images: Optional[torch.Tensor] = None\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        images_features = self.image_model(images) if images is not None else None\n",
    "        \n",
    "        \n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        cls_embeds = hidden_state[:, 0]  # (bs, dim) THIS IS THE CLS EMBEDDING\n",
    "        \n",
    "        \n",
    "        features = cls_embeds\n",
    "        if self.config.concat_mode == 'cls':\n",
    "            features = self.tabular_combiner(features,None,numeric_features)\n",
    "            features = torch.cat([f for f in [features,images_features] if f is not None], dim=-1)\n",
    "                \n",
    "        #features = self.ln(features)\n",
    "        features = self.pre_classifier(features)  # (bs, dim)\n",
    "        features = nn.ReLU()(features)  # (bs, dim)\n",
    "        features = self.dropout(features)  # (bs, dim)\n",
    "        \n",
    "        if self.config.concat_mode == 'dropout':\n",
    "            features = self.tabular_combiner(features,None,numeric_features)\n",
    "            features = torch.cat([f for f in [features,images_features] if f is not None], dim=-1)\n",
    "        \n",
    "        logits = self.classifier(features)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )\n",
    "\n",
    "def get_model(model_name,\n",
    "              seed,\n",
    "              num_numeric_features,\n",
    "              resnet_dropout,\n",
    "              seq_classif_dropout,\n",
    "              concat_mode,\n",
    "              num_image_features = 0,\n",
    "              problem_type = 'regression',\n",
    "              num_labels = 1,\n",
    "              combine_method = 'weighted_feature_sum_on_transformer_cat_and_numerical_feats'\n",
    "             ):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    config = DistilBertConfig.from_pretrained(model_name,\n",
    "                                              problem_type = problem_type,\n",
    "                                              num_labels = num_labels)\n",
    "    \n",
    "    tabular_config = TabularConfig(num_labels=num_labels,\n",
    "                               cat_feat_dim=0,\n",
    "                               numerical_feat_dim=5,\n",
    "                               combine_feat_method=combine_method,\n",
    "                               column_info=column_info_dict,\n",
    "                               task='regression')\n",
    "    \n",
    "    config.num_extra_features = num_numeric_features + num_image_features\n",
    "    \n",
    "    if combine_method=='weighted_feature_sum_on_transformer_cat_and_numerical_feats':\n",
    "        config.num_extra_features -= num_numeric_features\n",
    "        \n",
    "    \n",
    "    config.resnet_dropout = resnet_dropout\n",
    "    config.num_image_features = num_image_features\n",
    "    config.concat_mode = concat_mode\n",
    "    config.seq_classif_dropout = seq_classif_dropout\n",
    "\n",
    "    config.tabular_config = tabular_config\n",
    "    return AugmentedDistilBertForSequenceClassification(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae546c93-b621-40d6-912e-2b31be9170f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_info_dict = {\n",
    "    'text_cols': ['text_input'],\n",
    "    'num_cols': numeric_cols,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f98ecc-3441-49ef-90e2-ab516c4befbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dedication [SEP] A modern love story in which a misanthropic, emotionally complex author of a hit children's book is forced to team with a beautiful illustrator after his best friend and collaborator passes away. As Henry struggles with letting go of the ghosts of love and life, he discovers that sometimes you have to take a gamble at life to find love. [SEP] billy crudup, mandy moore, tom wilkinson [SEP] united states [SEP] justin theroux [SEP] comedy, drama, romance [SEP] english [SEP] first look international, hart-lunsford pictures, plum pictures\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of input to language model\n",
    "train['text_input'].iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4789b6f-a305-4c73-add7-9993975d0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictWriter:\n",
    "    \n",
    "    def __init__(self,file_path,field_names):\n",
    "        self.field_names = field_names\n",
    "        self.file_path = file_path\n",
    "        self.create_file() #Crerate file if it doesnt exist.\n",
    "        \n",
    "    def create_file(self):\n",
    "        if not os.path.exists(self.file_path):\n",
    "            print('creating file')\n",
    "            f = open(self.file_path, 'w')\n",
    "            w = csv.DictWriter(f, field_names)\n",
    "            w.writeheader()\n",
    "            f.close()\n",
    "        else:\n",
    "            print('file already exist. Will append rows to it.')\n",
    "            \n",
    "    def add_rows(self,rows):  \n",
    "        with open(self.file_path, 'a') as f:\n",
    "            w = csv.DictWriter(f,self.field_names)\n",
    "            for r in rows:    \n",
    "                w.writerow(r)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "324022a5-ccf6-4777-a908-ee186eb95083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/everything_as_text_and_images/distilbert-base-uncased-averageRating'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45229326-0b74-4930-b446-b6af38f6d2ae",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "236c755a-6e90-44c0-a215-3867575e0a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist. Will append rows to it.\n",
      "current best val score = inf\n",
      "training with following hparams:\n",
      "{'batch_size': 8,\n",
      " 'concat_mode': 'dropout',\n",
      " 'learning_rate': 1e-05,\n",
      " 'num_image_features': 512,\n",
      " 'repeats': 0,\n",
      " 'resnet_dropout': 0.5,\n",
      " 'weight_decay': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 23.298484802246094, 'eval_runtime': 11.2178, 'eval_samples_per_second': 156.359, 'eval_steps_per_second': 9.806, 'epoch': 0.1}\n",
      "{'eval_loss': 2.1109778881073, 'eval_runtime': 11.6703, 'eval_samples_per_second': 150.296, 'eval_steps_per_second': 9.426, 'epoch': 0.2}\n",
      "{'eval_loss': 1.5075581073760986, 'eval_runtime': 11.4282, 'eval_samples_per_second': 153.48, 'eval_steps_per_second': 9.625, 'epoch': 0.29}\n",
      "{'eval_loss': 1.4667749404907227, 'eval_runtime': 11.4736, 'eval_samples_per_second': 152.873, 'eval_steps_per_second': 9.587, 'epoch': 0.39}\n",
      "{'loss': 10.8814, 'learning_rate': 5.357917570498916e-06, 'epoch': 0.49}\n",
      "{'eval_loss': 1.4691433906555176, 'eval_runtime': 11.5029, 'eval_samples_per_second': 152.483, 'eval_steps_per_second': 9.563, 'epoch': 0.49}\n",
      "{'eval_loss': 1.3075448274612427, 'eval_runtime': 11.4503, 'eval_samples_per_second': 153.184, 'eval_steps_per_second': 9.607, 'epoch': 0.59}\n",
      "{'eval_loss': 1.466773509979248, 'eval_runtime': 11.5085, 'eval_samples_per_second': 152.409, 'eval_steps_per_second': 9.558, 'epoch': 0.68}\n",
      "{'eval_loss': 1.6364952325820923, 'eval_runtime': 11.4886, 'eval_samples_per_second': 152.674, 'eval_steps_per_second': 9.575, 'epoch': 0.78}\n",
      "{'eval_loss': 1.453407883644104, 'eval_runtime': 11.476, 'eval_samples_per_second': 152.841, 'eval_steps_per_second': 9.585, 'epoch': 0.88}\n",
      "{'loss': 1.3353, 'learning_rate': 9.950131597174125e-06, 'epoch': 0.98}\n",
      "{'eval_loss': 1.4281271696090698, 'eval_runtime': 11.4909, 'eval_samples_per_second': 152.642, 'eval_steps_per_second': 9.573, 'epoch': 0.98}\n",
      "{'eval_loss': 1.2552987337112427, 'eval_runtime': 11.444, 'eval_samples_per_second': 153.268, 'eval_steps_per_second': 9.612, 'epoch': 1.07}\n",
      "{'eval_loss': 1.246978998184204, 'eval_runtime': 11.4825, 'eval_samples_per_second': 152.754, 'eval_steps_per_second': 9.58, 'epoch': 1.17}\n",
      "{'eval_loss': 1.2776087522506714, 'eval_runtime': 11.4976, 'eval_samples_per_second': 152.554, 'eval_steps_per_second': 9.567, 'epoch': 1.27}\n",
      "{'eval_loss': 1.3807897567749023, 'eval_runtime': 11.5118, 'eval_samples_per_second': 152.366, 'eval_steps_per_second': 9.555, 'epoch': 1.37}\n",
      "{'loss': 1.281, 'learning_rate': 9.603823244216652e-06, 'epoch': 1.46}\n",
      "{'eval_loss': 1.3874444961547852, 'eval_runtime': 11.5001, 'eval_samples_per_second': 152.52, 'eval_steps_per_second': 9.565, 'epoch': 1.46}\n",
      "{'eval_loss': 1.2396948337554932, 'eval_runtime': 11.4748, 'eval_samples_per_second': 152.857, 'eval_steps_per_second': 9.586, 'epoch': 1.56}\n",
      "{'eval_loss': 1.2473185062408447, 'eval_runtime': 11.4559, 'eval_samples_per_second': 153.109, 'eval_steps_per_second': 9.602, 'epoch': 1.66}\n",
      "{'eval_loss': 1.285914659500122, 'eval_runtime': 11.4828, 'eval_samples_per_second': 152.751, 'eval_steps_per_second': 9.58, 'epoch': 1.76}\n",
      "{'eval_loss': 1.1905022859573364, 'eval_runtime': 11.4964, 'eval_samples_per_second': 152.569, 'eval_steps_per_second': 9.568, 'epoch': 1.86}\n",
      "{'loss': 1.2576, 'learning_rate': 9.25751489125918e-06, 'epoch': 1.95}\n",
      "{'eval_loss': 1.5952963829040527, 'eval_runtime': 11.4944, 'eval_samples_per_second': 152.596, 'eval_steps_per_second': 9.57, 'epoch': 1.95}\n",
      "{'eval_loss': 1.2815977334976196, 'eval_runtime': 11.504, 'eval_samples_per_second': 152.469, 'eval_steps_per_second': 9.562, 'epoch': 2.05}\n",
      "{'eval_loss': 1.107456922531128, 'eval_runtime': 11.4772, 'eval_samples_per_second': 152.825, 'eval_steps_per_second': 9.584, 'epoch': 2.15}\n",
      "{'eval_loss': 1.0067439079284668, 'eval_runtime': 11.4603, 'eval_samples_per_second': 153.05, 'eval_steps_per_second': 9.598, 'epoch': 2.25}\n",
      "{'eval_loss': 1.0214347839355469, 'eval_runtime': 11.485, 'eval_samples_per_second': 152.721, 'eval_steps_per_second': 9.578, 'epoch': 2.34}\n",
      "{'loss': 1.0417, 'learning_rate': 8.911206538301705e-06, 'epoch': 2.44}\n",
      "{'eval_loss': 1.1622371673583984, 'eval_runtime': 11.4953, 'eval_samples_per_second': 152.584, 'eval_steps_per_second': 9.569, 'epoch': 2.44}\n",
      "{'eval_loss': 1.0567055940628052, 'eval_runtime': 11.4707, 'eval_samples_per_second': 152.912, 'eval_steps_per_second': 9.59, 'epoch': 2.54}\n",
      "{'eval_loss': 1.0093770027160645, 'eval_runtime': 11.4962, 'eval_samples_per_second': 152.572, 'eval_steps_per_second': 9.568, 'epoch': 2.64}\n",
      "{'eval_loss': 1.0728635787963867, 'eval_runtime': 11.49, 'eval_samples_per_second': 152.655, 'eval_steps_per_second': 9.574, 'epoch': 2.73}\n",
      "{'eval_loss': 1.0041719675064087, 'eval_runtime': 11.4906, 'eval_samples_per_second': 152.647, 'eval_steps_per_second': 9.573, 'epoch': 2.83}\n",
      "{'loss': 0.9833, 'learning_rate': 8.564898185344232e-06, 'epoch': 2.93}\n",
      "{'eval_loss': 0.9708040952682495, 'eval_runtime': 11.5008, 'eval_samples_per_second': 152.511, 'eval_steps_per_second': 9.565, 'epoch': 2.93}\n",
      "{'eval_loss': 1.1409595012664795, 'eval_runtime': 11.4764, 'eval_samples_per_second': 152.836, 'eval_steps_per_second': 9.585, 'epoch': 3.03}\n",
      "{'eval_loss': 1.0163315534591675, 'eval_runtime': 11.4747, 'eval_samples_per_second': 152.858, 'eval_steps_per_second': 9.586, 'epoch': 3.12}\n",
      "{'eval_loss': 1.2034876346588135, 'eval_runtime': 11.4979, 'eval_samples_per_second': 152.549, 'eval_steps_per_second': 9.567, 'epoch': 3.22}\n",
      "{'eval_loss': 1.1611474752426147, 'eval_runtime': 11.4778, 'eval_samples_per_second': 152.817, 'eval_steps_per_second': 9.584, 'epoch': 3.32}\n",
      "{'loss': 0.8519, 'learning_rate': 8.218589832386758e-06, 'epoch': 3.42}\n",
      "{'eval_loss': 1.0934853553771973, 'eval_runtime': 11.4882, 'eval_samples_per_second': 152.678, 'eval_steps_per_second': 9.575, 'epoch': 3.42}\n",
      "{'eval_loss': 1.043751835823059, 'eval_runtime': 11.5128, 'eval_samples_per_second': 152.352, 'eval_steps_per_second': 9.555, 'epoch': 3.52}\n",
      "{'eval_loss': 1.0029538869857788, 'eval_runtime': 11.4724, 'eval_samples_per_second': 152.889, 'eval_steps_per_second': 9.588, 'epoch': 3.61}\n",
      "{'eval_loss': 0.9490728378295898, 'eval_runtime': 11.4955, 'eval_samples_per_second': 152.582, 'eval_steps_per_second': 9.569, 'epoch': 3.71}\n",
      "{'eval_loss': 0.9459097385406494, 'eval_runtime': 11.4997, 'eval_samples_per_second': 152.526, 'eval_steps_per_second': 9.565, 'epoch': 3.81}\n",
      "{'loss': 0.7965, 'learning_rate': 7.872281479429285e-06, 'epoch': 3.91}\n",
      "{'eval_loss': 1.0917556285858154, 'eval_runtime': 11.4984, 'eval_samples_per_second': 152.542, 'eval_steps_per_second': 9.567, 'epoch': 3.91}\n",
      "{'eval_loss': 0.9610134959220886, 'eval_runtime': 11.4832, 'eval_samples_per_second': 152.745, 'eval_steps_per_second': 9.579, 'epoch': 4.0}\n",
      "{'eval_loss': 1.3517484664916992, 'eval_runtime': 11.49, 'eval_samples_per_second': 152.654, 'eval_steps_per_second': 9.574, 'epoch': 4.1}\n",
      "{'eval_loss': 1.6976581811904907, 'eval_runtime': 11.4845, 'eval_samples_per_second': 152.728, 'eval_steps_per_second': 9.578, 'epoch': 4.2}\n",
      "{'eval_loss': 1.0434759855270386, 'eval_runtime': 11.4738, 'eval_samples_per_second': 152.87, 'eval_steps_per_second': 9.587, 'epoch': 4.3}\n",
      "{'loss': 0.6739, 'learning_rate': 7.5259731264718115e-06, 'epoch': 4.39}\n",
      "{'eval_loss': 1.058801293373108, 'eval_runtime': 11.4647, 'eval_samples_per_second': 152.992, 'eval_steps_per_second': 9.595, 'epoch': 4.39}\n",
      "{'eval_loss': 1.0298759937286377, 'eval_runtime': 11.4956, 'eval_samples_per_second': 152.58, 'eval_steps_per_second': 9.569, 'epoch': 4.49}\n",
      "{'eval_loss': 1.2233846187591553, 'eval_runtime': 11.4749, 'eval_samples_per_second': 152.856, 'eval_steps_per_second': 9.586, 'epoch': 4.59}\n",
      "{'eval_loss': 1.1365629434585571, 'eval_runtime': 11.4952, 'eval_samples_per_second': 152.585, 'eval_steps_per_second': 9.569, 'epoch': 4.69}\n",
      "{'eval_loss': 1.0459157228469849, 'eval_runtime': 11.4692, 'eval_samples_per_second': 152.931, 'eval_steps_per_second': 9.591, 'epoch': 4.79}\n",
      "{'loss': 0.6628, 'learning_rate': 7.179664773514338e-06, 'epoch': 4.88}\n",
      "{'eval_loss': 1.1296659708023071, 'eval_runtime': 11.4925, 'eval_samples_per_second': 152.622, 'eval_steps_per_second': 9.571, 'epoch': 4.88}\n",
      "{'eval_loss': 1.5901143550872803, 'eval_runtime': 11.4721, 'eval_samples_per_second': 152.893, 'eval_steps_per_second': 9.588, 'epoch': 4.98}\n",
      "{'eval_loss': 0.9639193415641785, 'eval_runtime': 11.4722, 'eval_samples_per_second': 152.891, 'eval_steps_per_second': 9.588, 'epoch': 5.08}\n",
      "{'eval_loss': 1.234055995941162, 'eval_runtime': 11.4835, 'eval_samples_per_second': 152.741, 'eval_steps_per_second': 9.579, 'epoch': 5.18}\n",
      "{'eval_loss': 1.0500229597091675, 'eval_runtime': 11.4514, 'eval_samples_per_second': 153.169, 'eval_steps_per_second': 9.606, 'epoch': 5.27}\n",
      "{'loss': 0.6348, 'learning_rate': 6.833356420556864e-06, 'epoch': 5.37}\n",
      "{'eval_loss': 0.9986048340797424, 'eval_runtime': 11.4921, 'eval_samples_per_second': 152.627, 'eval_steps_per_second': 9.572, 'epoch': 5.37}\n",
      "{'eval_loss': 0.9957428574562073, 'eval_runtime': 11.4894, 'eval_samples_per_second': 152.663, 'eval_steps_per_second': 9.574, 'epoch': 5.47}\n",
      "{'eval_loss': 1.0599453449249268, 'eval_runtime': 11.4463, 'eval_samples_per_second': 153.237, 'eval_steps_per_second': 9.61, 'epoch': 5.57}\n",
      "{'eval_loss': 0.9765678644180298, 'eval_runtime': 11.4749, 'eval_samples_per_second': 152.855, 'eval_steps_per_second': 9.586, 'epoch': 5.66}\n",
      "{'eval_loss': 1.013008713722229, 'eval_runtime': 11.4925, 'eval_samples_per_second': 152.622, 'eval_steps_per_second': 9.571, 'epoch': 5.76}\n",
      "{'loss': 0.5929, 'learning_rate': 6.487048067599391e-06, 'epoch': 5.86}\n",
      "{'eval_loss': 1.0812171697616577, 'eval_runtime': 11.4727, 'eval_samples_per_second': 152.884, 'eval_steps_per_second': 9.588, 'epoch': 5.86}\n",
      "{'eval_loss': 1.088429570198059, 'eval_runtime': 11.4574, 'eval_samples_per_second': 153.089, 'eval_steps_per_second': 9.601, 'epoch': 5.96}\n",
      "{'eval_loss': 1.0435523986816406, 'eval_runtime': 11.511, 'eval_samples_per_second': 152.376, 'eval_steps_per_second': 9.556, 'epoch': 6.05}\n",
      "{'eval_loss': 0.9672557711601257, 'eval_runtime': 11.4928, 'eval_samples_per_second': 152.618, 'eval_steps_per_second': 9.571, 'epoch': 6.15}\n",
      "{'eval_loss': 1.1504744291305542, 'eval_runtime': 11.4856, 'eval_samples_per_second': 152.713, 'eval_steps_per_second': 9.577, 'epoch': 6.25}\n",
      "{'loss': 0.5501, 'learning_rate': 6.141432331347833e-06, 'epoch': 6.35}\n",
      "{'eval_loss': 1.283210277557373, 'eval_runtime': 11.493, 'eval_samples_per_second': 152.615, 'eval_steps_per_second': 9.571, 'epoch': 6.35}\n",
      "{'eval_loss': 1.1511510610580444, 'eval_runtime': 11.4773, 'eval_samples_per_second': 152.823, 'eval_steps_per_second': 9.584, 'epoch': 6.45}\n",
      "{'eval_loss': 1.050164818763733, 'eval_runtime': 11.4727, 'eval_samples_per_second': 152.885, 'eval_steps_per_second': 9.588, 'epoch': 6.54}\n",
      "{'eval_loss': 0.9957234859466553, 'eval_runtime': 11.4741, 'eval_samples_per_second': 152.866, 'eval_steps_per_second': 9.587, 'epoch': 6.64}\n",
      "{'eval_loss': 0.9924091100692749, 'eval_runtime': 11.4955, 'eval_samples_per_second': 152.582, 'eval_steps_per_second': 9.569, 'epoch': 6.74}\n",
      "{'train_runtime': 2318.9347, 'train_samples_per_second': 52.951, 'train_steps_per_second': 6.624, 'train_loss': 1.5900070898083674, 'epoch': 6.74}\n",
      "{'eval_loss': 0.9459097385406494, 'eval_runtime': 11.5225, 'eval_samples_per_second': 152.223, 'eval_steps_per_second': 9.547, 'epoch': 6.74}\n",
      "got a better model, with score 0.9459 saving...\n",
      "saved\n",
      "training with following hparams:\n",
      "{'batch_size': 8,\n",
      " 'concat_mode': 'dropout',\n",
      " 'learning_rate': 1e-05,\n",
      " 'num_image_features': 512,\n",
      " 'repeats': 0,\n",
      " 'resnet_dropout': 0.5,\n",
      " 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 23.298095703125, 'eval_runtime': 11.5046, 'eval_samples_per_second': 152.461, 'eval_steps_per_second': 9.561, 'epoch': 0.1}\n",
      "{'eval_loss': 2.111063003540039, 'eval_runtime': 11.4507, 'eval_samples_per_second': 153.179, 'eval_steps_per_second': 9.606, 'epoch': 0.2}\n",
      "{'eval_loss': 1.5084279775619507, 'eval_runtime': 11.4828, 'eval_samples_per_second': 152.75, 'eval_steps_per_second': 9.58, 'epoch': 0.29}\n",
      "{'eval_loss': 1.4638259410858154, 'eval_runtime': 11.5001, 'eval_samples_per_second': 152.521, 'eval_steps_per_second': 9.565, 'epoch': 0.39}\n",
      "{'loss': 10.8824, 'learning_rate': 5.357917570498916e-06, 'epoch': 0.49}\n",
      "{'eval_loss': 1.4704539775848389, 'eval_runtime': 11.5033, 'eval_samples_per_second': 152.478, 'eval_steps_per_second': 9.562, 'epoch': 0.49}\n",
      "{'eval_loss': 1.3102996349334717, 'eval_runtime': 11.4749, 'eval_samples_per_second': 152.856, 'eval_steps_per_second': 9.586, 'epoch': 0.59}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1514/2396666336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1514/1017180466.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, numeric_features, images)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mimages_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "num_evals = 20\n",
    "patience = 2 if DEBUG else 30\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "eval_steps = 50 if DEBUG else 100\n",
    "\n",
    "\n",
    "hparams = {'batch_size' : [8,16,32],\n",
    "           'augment_images':[True,False],\n",
    "           'learning_rate' : [1e-5, 2e-5, 3e-5,5e-5],\n",
    "           'weight_decay' : [0.1,0.01],\n",
    "           'resnet_dropout':[0.5],\n",
    "           'concat_mode':['dropout'],\n",
    "           'num_image_features':[2**9],\n",
    "           'repeats': range(1)}\n",
    "\n",
    "\n",
    "combs = list(product(*[range(len(i)) for i in list(hparams.values())]))\n",
    "scores = np.zeros([len(i) for i in list(hparams.values())])\n",
    "\n",
    "\n",
    "\n",
    "#trials_df_rows = []\n",
    "\n",
    "field_names = list(hparams.keys()) + ['score']\n",
    "dw = DictWriter(TRIALS_DF_PATH,field_names)\n",
    "\n",
    "currernt_trials_df = pd.read_csv(TRIALS_DF_PATH) #This can be empty or not.\n",
    "done_trials = currernt_trials_df.drop('score',axis=1).to_dict(orient='records') #empty list or not\n",
    "best_score = min(float('inf'),currernt_trials_df['score'].min())\n",
    "\n",
    "print(f'current best val score = {best_score}')\n",
    "\n",
    "for idx,comb_indexes in enumerate(combs):\n",
    "    comb_values = {name:val[idx] for name,val,idx in zip(hparams.keys(),hparams.values(),comb_indexes)}\n",
    "    \n",
    "    \n",
    "    if comb_values not in done_trials: #Check if trial alrready exists. If it does, skip.\n",
    "        print('training with following hparams:')\n",
    "        pprint(comb_values)\n",
    "\n",
    "        training_args = TrainingArguments(output_dir=f\"{MODEL_NAME}-{TARGET_COL}\",\n",
    "                                          per_device_train_batch_size = comb_values['batch_size'],\n",
    "                                          learning_rate=comb_values['learning_rate'],\n",
    "                                          weight_decay=comb_values['weight_decay'],\n",
    "                                          seed = 42,\n",
    "                                          fp16=True,\n",
    "                                          per_device_eval_batch_size = 16,\n",
    "                                          warmup_ratio=0.06,\n",
    "                                          num_train_epochs = epochs,\n",
    "                                          evaluation_strategy = \"steps\",\n",
    "                                          save_strategy = \"steps\",\n",
    "                                          load_best_model_at_end=True,\n",
    "                                          eval_steps = eval_steps,\n",
    "                                          save_steps = eval_steps,\n",
    "                                          save_total_limit = 1,\n",
    "                                          log_level = 'error',\n",
    "                                          disable_tqdm = True\n",
    "\n",
    "                                        )\n",
    "\n",
    "        multi_modal_model = get_model(model_name = MODEL_NAME,\n",
    "                             seed = training_args.seed,\n",
    "                             num_numeric_features = len(numeric_cols),\n",
    "                             resnet_dropout = comb_values['resnet_dropout'],\n",
    "                             concat_mode = comb_values['concat_mode'],\n",
    "                             num_image_features = comb_values['num_image_features'])\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=multi_modal_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        score = trainer.evaluate()['eval_loss']\n",
    "\n",
    "        comb_values['score'] = score\n",
    "\n",
    "        dw.add_rows([comb_values]) #Append to dataframe\n",
    "\n",
    "        #trials_df_rows.append(comb_values)\n",
    "\n",
    "        if score<best_score:\n",
    "            print(f'got a better model, with score {np.round(score,4)} saving...')\n",
    "            best_score = score\n",
    "            trainer.save_model(FINAL_MODEL_PATH)\n",
    "            \n",
    "            print('saved')\n",
    "    else:\n",
    "        print('skipping trial because already exists')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec053f-161a-49e8-8245-4ed9ac82f4de",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a2e583d-0816-45c0-bd8a-d0a6ee99dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomSearch:\n",
    "    def __init__(self,tried_hparams = []):\n",
    "        \n",
    "        self.tried_hparams = tried_hparams\n",
    "        \n",
    "    def get_rand_comb_value(self):\n",
    "        \n",
    "        space = {'batch_size' : int(np.random.choice([8,16])),\n",
    "                 'augment_images':False,\n",
    "                 'learning_rate' : float(np.random.choice([2e-5, 3e-5,5e-5,7e-5,1e-4,4e-4,1e-3])),#10**(-np.random.uniform(4,5.5)),#\n",
    "                 'weight_decay' : float(np.random.choice([0.1,0.01,0.001])),\n",
    "                 'resnet_dropout':float(np.random.choice(np.arange(0,.6,.1))),\n",
    "                 'seq_classif_dropout':0.2,\n",
    "                 'concat_mode':str(np.random.choice(['dropout','cls'])),\n",
    "                 'num_image_features':int(np.random.choice(2**np.arange(2,10))),\n",
    "                 'repeats': 0}\n",
    "        \n",
    "\n",
    "        if space not in self.tried_hparams:\n",
    "            self.tried_hparams.append(space)\n",
    "            return space\n",
    "\n",
    "        return self.get_rand_comb_value()\n",
    "\n",
    "    \n",
    "def get_current_trials(trials_df_path = TRIALS_DF_PATH):    \n",
    "    currernt_trials_df = pd.read_csv(trials_df_path) #This can be empty or not.\n",
    "    done_trials = currernt_trials_df.drop('score',axis=1).to_dict(orient='records') #empty list or not\n",
    "    best_score = min(float('inf'),currernt_trials_df['score'].min())\n",
    "    return done_trials,best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3f8537-898f-4623-8962-46286254cc99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist. Will append rows to it.\n",
      "current best val score = 6.343607425689697\n",
      "Trial 38:\n",
      "\n",
      "training with following hparams:\n",
      "{'augment_images': False,\n",
      " 'batch_size': 8,\n",
      " 'concat_mode': 'dropout',\n",
      " 'learning_rate': 0.001,\n",
      " 'num_image_features': 16,\n",
      " 'repeats': 0,\n",
      " 'resnet_dropout': 0.2,\n",
      " 'seq_classif_dropout': 0.2,\n",
      " 'weight_decay': 0.01}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6359/2966411439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m                              \u001b[0mseq_classif_dropout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcomb_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_classif_dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                              \u001b[0mconcat_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomb_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'concat_mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                              num_image_features = comb_values['num_image_features'])\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         trainer = Trainer(\n",
      "\u001b[0;32m/tmp/ipykernel_6359/2024080997.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model_name, seed, num_numeric_features, resnet_dropout, seq_classif_dropout, concat_mode, num_image_features, problem_type, num_labels, combine_method)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabular_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabular_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mAugmentedDistilBertForSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6359/2024080997.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m#set_parameter_requires_grad(self.model, feature_extract)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mresnet18\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplays\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resnet18\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_resnet\u001b[0;34m(arch, block, layers, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_zip_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1 if DEBUG else 15\n",
    "num_evals = 20\n",
    "patience = 1 if DEBUG else 30\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "eval_steps = 50 if DEBUG else 100\n",
    "num_trials = 200\n",
    "\n",
    "field_names = list(RandomSearch().get_rand_comb_value().keys()) + ['score']\n",
    "\n",
    "dw = DictWriter(TRIALS_DF_PATH,field_names)\n",
    "done_trials,best_score = get_current_trials()\n",
    "RS = RandomSearch(tried_hparams = list(done_trials))\n",
    "\n",
    "print(f'current best val score = {best_score}')\n",
    "\n",
    "\n",
    "remaining_trials = range(len(done_trials),num_trials)\n",
    "all_combs = [RS.get_rand_comb_value() for _ in range(len(done_trials),num_trials)]\n",
    "\n",
    "for trial,comb_values in zip(remaining_trials,all_combs):\n",
    "    print(f'Trial {trial+1}:\\n')\n",
    "    \n",
    "    #comb_values = RS.get_rand_comb_value()    \n",
    "    \n",
    "\n",
    "    \n",
    "    if comb_values not in done_trials: #Check if trial alrready exists. If it does, skip.\n",
    "        print('training with following hparams:')\n",
    "        pprint(comb_values)\n",
    "        print('\\n')\n",
    "\n",
    "        training_args = TrainingArguments(output_dir=f\"{MODEL_NAME}-{TARGET_COL}\",\n",
    "                                          per_device_train_batch_size = comb_values['batch_size'],\n",
    "                                          learning_rate=comb_values['learning_rate'],\n",
    "                                          weight_decay=comb_values['weight_decay'],\n",
    "                                          seed = 42,\n",
    "                                          fp16=True,\n",
    "                                          per_device_eval_batch_size = 16,\n",
    "                                          warmup_ratio=0.06,\n",
    "                                          num_train_epochs = epochs,\n",
    "                                          evaluation_strategy = \"steps\",\n",
    "                                          save_strategy = \"steps\",\n",
    "                                          load_best_model_at_end=True,\n",
    "                                          eval_steps = eval_steps,\n",
    "                                          save_steps = eval_steps,\n",
    "                                          save_total_limit = 1,\n",
    "                                          log_level = 'error',\n",
    "                                          disable_tqdm = True\n",
    "\n",
    "                                        )\n",
    "\n",
    "        multi_modal_model = get_model(model_name = MODEL_NAME,\n",
    "                             seed = training_args.seed,\n",
    "                             num_numeric_features = len(numeric_cols),\n",
    "                             resnet_dropout = comb_values['resnet_dropout'],\n",
    "                             seq_classif_dropout= comb_values['seq_classif_dropout'],\n",
    "                             concat_mode = comb_values['concat_mode'],\n",
    "                             num_image_features = comb_values['num_image_features'])\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=multi_modal_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset_augmented if comb_values['augment_images'] else train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        score = trainer.evaluate()['eval_loss']\n",
    "\n",
    "        comb_values['score'] = score\n",
    "        \n",
    "        if not DEBUG:\n",
    "            dw.add_rows([comb_values]) #Append to dataframe\n",
    "\n",
    "        #trials_df_rows.append(comb_values)\n",
    "\n",
    "        if score<best_score:\n",
    "            print(f'got a better model, with score {np.round(score,4)} saving...')\n",
    "            best_score = score\n",
    "            \n",
    "            if not DEBUG:\n",
    "                trainer.save_model(FINAL_MODEL_PATH)\n",
    "            \n",
    "            print('saved')\n",
    "    else:\n",
    "        print('skipping trial because already exists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f39c78f-cc7a-4c0c-b678-611ef3e9e714",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertConfig' object has no attribute 'tabular_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24250/96016913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m best_model = AugmentedDistilBertForSequenceClassification.from_pretrained(FINAL_MODEL_PATH,\n\u001b[1;32m      3\u001b[0m                                                                           \u001b[0mproblem_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'regression'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                                           num_labels=1)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrainer_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mno_init_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_enable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1493\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24250/2339618998.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtabular_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabular_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertConfig' object has no attribute 'tabular_config'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Test set performance\n",
    "best_model = AugmentedDistilBertForSequenceClassification.from_pretrained(FINAL_MODEL_PATH,\n",
    "                                                                          problem_type='regression',\n",
    "                                                                          num_labels=1)\n",
    "trainer_best_model = Trainer(model=best_model)\n",
    "\n",
    "predictions = trainer_best_model.predict(test_dataset)\n",
    "preds = predictions.predictions.flatten() \n",
    "labels = predictions.label_ids\n",
    "\n",
    "if TARGET_COL == 'revenue_worldwide_BOM':\n",
    "    preds = np.expm1(preds)\n",
    "    labels = np.expm1(labels)\n",
    "\n",
    "\n",
    "mse = ((preds-labels)**2).mean()\n",
    "mae = (np.abs(preds-labels)).mean()\n",
    "\n",
    "errors = {'MAE':mae,'MSE':mse,'RMSE':np.sqrt(mse)}\n",
    "pd.DataFrame([errors]).to_csv(TEST_PERFORMANCE_PATH,\n",
    "                              index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38e5dd30-4c56-4911-8944-f4d88e076e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/everything_as_text_and_images/distilbert-base-uncased-revenue_worldwide_BOM'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42f22e29-ddf6-4342-81c3-4f32201c0819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.65875113, 'MSE': 0.7385504, 'RMSE': 0.85938954}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbd0cafa-171b-4183-b8f2-154434e1b464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   1.,   1.,   0.,   3.,   5.,   8.,   7.,   8.,  10.,  20.,\n",
       "         25.,  25.,  26.,  32.,  48.,  48.,  63.,  61.,  90.,  99.,  95.,\n",
       "        112., 147., 111., 104.,  74.,  75.,  66.,  61.,  67.,  66.,  74.,\n",
       "         64.,  31.,  15.,   6.,   2.,   1.,   2.]),\n",
       " array([1.6255612, 1.6396927, 1.6538242, 1.6679556, 1.6820871, 1.6962186,\n",
       "        1.71035  , 1.7244815, 1.738613 , 1.7527444, 1.766876 , 1.7810074,\n",
       "        1.7951388, 1.8092704, 1.8234018, 1.8375332, 1.8516648, 1.8657962,\n",
       "        1.8799276, 1.8940592, 1.9081906, 1.922322 , 1.9364536, 1.950585 ,\n",
       "        1.9647164, 1.978848 , 1.9929794, 2.0071108, 2.0212424, 2.035374 ,\n",
       "        2.0495052, 2.0636368, 2.0777683, 2.0918996, 2.1060312, 2.1201627,\n",
       "        2.134294 , 2.1484256, 2.1625571, 2.1766884, 2.19082  ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 40 artists>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "plt.hist(np.log1p(labels),bins=40)\n",
    "plt.hist(np.log1p(preds),bins=40)\n",
    "\n",
    "#plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c45f52b8-5ff5-42f2-9e56-c4cf427ba32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   4.,  17.,  34.,  53.,  64.,  70., 102.,  81.,  89.,  91.,\n",
       "         84.,  74.,  73.,  76.,  55.,  56.,  51.,  48.,  41.,  26.,  30.,\n",
       "         31.,  20.,  27.,  29.,  21.,  24.,  20.,  14.,  30.,  20.,  27.,\n",
       "         24.,  31.,  22.,  27.,  52.,  45.,  69.]),\n",
       " array([ 8.904737 ,  9.151753 ,  9.398769 ,  9.645784 ,  9.8928   ,\n",
       "        10.139816 , 10.386832 , 10.633848 , 10.880863 , 11.127879 ,\n",
       "        11.374895 , 11.621911 , 11.868927 , 12.115942 , 12.362958 ,\n",
       "        12.609974 , 12.85699  , 13.104006 , 13.351021 , 13.598037 ,\n",
       "        13.845053 , 14.092069 , 14.339085 , 14.5861   , 14.833116 ,\n",
       "        15.080132 , 15.3271475, 15.574163 , 15.821178 , 16.068195 ,\n",
       "        16.31521  , 16.562225 , 16.809242 , 17.056257 , 17.303274 ,\n",
       "        17.55029  , 17.797304 , 18.044321 , 18.291336 , 18.538353 ,\n",
       "        18.785368 ], dtype=float32),\n",
       " <BarContainer object of 40 artists>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "plt.hist(np.log1p(labels),bins=40)\n",
    "plt.hist(np.log1p(preds),bins=40)\n",
    "\n",
    "#plt.xscale('log')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tests roberta.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e0d67806dc4a8e993cfd9f5a451bfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06b2527e9e904b5199a36d2216033e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ab61b2ec2054e61becc95bc2187b62d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f0136838a2b44dfb71e8d7f98cc374a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e32e63ab861449adb2f5d9a31d5785ff",
       "IPY_MODEL_bd10fe03b50d4f2fb4911166e4219b18",
       "IPY_MODEL_b32718625f18463ebb6df599fa5dc30c"
      ],
      "layout": "IPY_MODEL_63ea088d311c4229b2f045cb2d1b5d15"
     }
    },
    "128d7ae29eb74e469d25af1941d13c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15a7b28383b340bbb6201f026d642410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d376fbfd41f42c9abf5206021234669": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22fff9b519434cfe8644ee9478c23285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a189fac46b0648d0924bbe6ac7b8036a",
      "placeholder": "",
      "style": "IPY_MODEL_15a7b28383b340bbb6201f026d642410",
      "value": " 226k/226k [00:00&lt;00:00, 1.07MB/s]"
     }
    },
    "25b66346a56143cc921abe4556a9f6a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e3b82dda5fa468d851c065a6cb538c0",
      "placeholder": "",
      "style": "IPY_MODEL_a43b2fb60dba45e4aa831ac03bb99323",
      "value": "Downloading: 100%"
     }
    },
    "29a5a5add01042c588faaf5751b5901e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b15021c19ee4608941e9c340af1fc94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b1d2bd0e14043f781d15615263b64ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29a5a5add01042c588faaf5751b5901e",
      "placeholder": "",
      "style": "IPY_MODEL_5c9ea04fc2524e6694b88fc6cda31ff8",
      "value": "Downloading: 100%"
     }
    },
    "518d709c96a14796b48c0ec8fdce9bbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_128d7ae29eb74e469d25af1941d13c7d",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3023c4315d8401e8c8b886ae82ef3ca",
      "value": 483
     }
    },
    "53c58fddfd8140a38c81c77f0726c864": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25b66346a56143cc921abe4556a9f6a3",
       "IPY_MODEL_d3b13cad58694f2aa1400827e8a7f619",
       "IPY_MODEL_c12944a1bd2541bbb210cb1c3585c855"
      ],
      "layout": "IPY_MODEL_02e0d67806dc4a8e993cfd9f5a451bfa"
     }
    },
    "5c9ea04fc2524e6694b88fc6cda31ff8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63ea088d311c4229b2f045cb2d1b5d15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b5a84553cdf4c2abdc9f817f5c49c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e3b82dda5fa468d851c065a6cb538c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7158cba3ef694c99a79339987df170c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b5a84553cdf4c2abdc9f817f5c49c32",
      "placeholder": "",
      "style": "IPY_MODEL_7503acc7897e4264bcc1e50bd683da3a",
      "value": "Downloading: 100%"
     }
    },
    "7503acc7897e4264bcc1e50bd683da3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76eca6e8d5454b4a9693974954e60c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77b1c79a951d42d8a79ee3c472852192": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "793ca2e7b3b24100ba6fa5551d44e03a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e42e1172adb461d871c7676afde511a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "977047c3c9b0478fa2129dfb22504fb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9beb6465dfc3494aaafaec4d1c02fa12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a189fac46b0648d0924bbe6ac7b8036a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43b2fb60dba45e4aa831ac03bb99323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa8df3ea2fb54cd2b9e2882e0649ee98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2cd9e361f404c15ab7b85e343f04176": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b834b852125147bdb5671574be4f1a0a",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b15021c19ee4608941e9c340af1fc94",
      "value": 231508
     }
    },
    "b32718625f18463ebb6df599fa5dc30c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ab61b2ec2054e61becc95bc2187b62d",
      "placeholder": "",
      "style": "IPY_MODEL_76eca6e8d5454b4a9693974954e60c9f",
      "value": " 455k/455k [00:00&lt;00:00, 1.94MB/s]"
     }
    },
    "b66319ac17e649aaa314f8e83cf7543c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b834b852125147bdb5671574be4f1a0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd10fe03b50d4f2fb4911166e4219b18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_977047c3c9b0478fa2129dfb22504fb8",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9beb6465dfc3494aaafaec4d1c02fa12",
      "value": 466062
     }
    },
    "c12944a1bd2541bbb210cb1c3585c855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77b1c79a951d42d8a79ee3c472852192",
      "placeholder": "",
      "style": "IPY_MODEL_dacaefdf213d4b6ca041b7d491b94d42",
      "value": " 28.0/28.0 [00:00&lt;00:00, 323B/s]"
     }
    },
    "d009bc9fcbf54d089b779bd1adc4f49b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d367c07d6c594fdf80e0bfc60a339504": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3b13cad58694f2aa1400827e8a7f619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d009bc9fcbf54d089b779bd1adc4f49b",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06b2527e9e904b5199a36d2216033e25",
      "value": 28
     }
    },
    "dacaefdf213d4b6ca041b7d491b94d42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e127c609a823462aab9318fd04bda74b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b1d2bd0e14043f781d15615263b64ec",
       "IPY_MODEL_518d709c96a14796b48c0ec8fdce9bbc",
       "IPY_MODEL_feb6afab6c2247d48db7ba792d1daf85"
      ],
      "layout": "IPY_MODEL_aa8df3ea2fb54cd2b9e2882e0649ee98"
     }
    },
    "e32e63ab861449adb2f5d9a31d5785ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_793ca2e7b3b24100ba6fa5551d44e03a",
      "placeholder": "",
      "style": "IPY_MODEL_b66319ac17e649aaa314f8e83cf7543c",
      "value": "Downloading: 100%"
     }
    },
    "f3023c4315d8401e8c8b886ae82ef3ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4c6a7d98e284719999c9d4d2c5ff366": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7158cba3ef694c99a79339987df170c5",
       "IPY_MODEL_b2cd9e361f404c15ab7b85e343f04176",
       "IPY_MODEL_22fff9b519434cfe8644ee9478c23285"
      ],
      "layout": "IPY_MODEL_7e42e1172adb461d871c7676afde511a"
     }
    },
    "feb6afab6c2247d48db7ba792d1daf85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d376fbfd41f42c9abf5206021234669",
      "placeholder": "",
      "style": "IPY_MODEL_d367c07d6c594fdf80e0bfc60a339504",
      "value": " 483/483 [00:00&lt;00:00, 11.2kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
